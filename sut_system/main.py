import os
import re
import traceback
import time
import asyncio
import sys

# --- å¿…è¦çš„å¥—ä»¶å¼•å…¥ ---
from dotenv import load_dotenv
import jieba
# ä¿®æ­£ï¼šæ­£ç¢ºå¼•å…¥ ChatGroq å’Œç›¸é—œæ¨¡çµ„
from langchain_groq import ChatGroq
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser


class SOPQuerySystem:
    """
    å°‡æ•´å€‹ SOP æŸ¥è©¢æµç¨‹å°è£åœ¨ä¸€å€‹é¡åˆ¥ä¸­ï¼Œæ–¹ä¾¿ç®¡ç†ç‹€æ…‹èˆ‡è¨­å®šã€‚
    """
    def __init__(self):
        """åˆå§‹åŒ–ç³»çµ±ï¼Œè¼‰å…¥è¨­å®šã€LLM å’Œæ–‡ä»¶ã€‚"""
        print("--- é–‹å§‹åˆå§‹åŒ– SOP æŸ¥è©¢ç³»çµ± ---")
        self._load_config()
        self.llm = None
        self.sections_to_search = []
        self.initialization_success = self._initialize()

    def _load_config(self):
        """è¼‰å…¥æ‰€æœ‰è¨­å®šæª”"""
        load_dotenv()
        self.config = {
            "MODEL_NAME": os.getenv("MODEL_NAME", "llama3-8b-8192"),
            "GROQ_API_KEY": os.getenv("GROQ_API_KEY"),
            "SIMPLIFIED_MD_FILENAME": os.getenv("SIMPLIFIED_MD_FILENAME", "simplified_output_by_section.md"),
            "TARGET_DESCRIPTION_KEYWORDS": ["çµå¡Š", "éç¯©", "é †åº", "å¸æ¿•", "ç¨ åº¦", "é»ç¨ ", "æµå‹•æ€§"],
            "CHINESE_STOP_WORDS": {"çš„", "å’Œ", "èˆ‡", "æˆ–", "äº†", "å‘¢", "å—", "å–”", "å•Š", "é—œæ–¼", "æœ‰é—œ", "è«‹", "è«‹å•", " ", ""},
            "ALLOWED_WORKSHEET_IDENTIFIERS": ["å·¥ä½œè¡¨: 9", "å·¥ä½œè¡¨: 10"]
        }

    def _initialize(self):
        """åŸ·è¡Œåˆå§‹åŒ–æ­¥é©Ÿï¼šè¨­å®š LLM å’Œè¼‰å…¥æ–‡ä»¶ã€‚"""
        # 1. åˆå§‹åŒ– LangChain çš„ ChatGroq ç‰©ä»¶
        if not self.config["GROQ_API_KEY"] or not self.config["MODEL_NAME"]:
            print("âŒ éŒ¯èª¤ï¼šæœªèƒ½ç²å– GROQ_API_KEY æˆ– MODEL_NAMEï¼Œç„¡æ³•åˆå§‹åŒ– ChatGroqã€‚")
            return False
        try:
            self.llm = ChatGroq(model=self.config["MODEL_NAME"], groq_api_key=self.config["GROQ_API_KEY"])
            print(f"âœ… ChatGroq (LangChain) for model '{self.config['MODEL_NAME']}' åˆå§‹åŒ–æˆåŠŸã€‚")
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ– ChatGroq æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            return False

        # 2. è¼‰å…¥ä¸¦éæ¿¾ SOP æ–‡ä»¶å€å¡Š
        all_sections = self._load_markdown_sections()
        if not all_sections:
            print(f"âŒ éŒ¯èª¤ï¼šæœªèƒ½å¾ '{self.config['SIMPLIFIED_MD_FILENAME']}' è¼‰å…¥ä»»ä½• SOP æ–‡ä»¶å€å¡Šã€‚")
            return False

        self.sections_to_search = self._filter_sections_by_title(all_sections)
        if not self.sections_to_search:
            print(f"âš ï¸ è­¦å‘Šï¼šæœªéæ¿¾å‡ºä»»ä½•ç›®æ¨™å€å¡Šï¼Œå°‡åœ¨å…¨éƒ¨ {len(all_sections)} å€‹å€å¡Šä¸­æœå°‹ã€‚")
            self.sections_to_search = all_sections
        
        print(f"âœ… æˆåŠŸæº–å‚™ {len(self.sections_to_search)} å€‹å€å¡Šä¾›æŸ¥è©¢ã€‚")
        return True

    def _load_markdown_sections(self):
        """å¾æª”æ¡ˆè®€å–ä¸¦è§£æ Markdown å€å¡Šã€‚"""
        filename = self.config["SIMPLIFIED_MD_FILENAME"]
        print(f"æ­£åœ¨è¼‰å…¥æª”æ¡ˆ: {filename}")
        if not os.path.exists(filename):
            print(f"âŒ éŒ¯èª¤ï¼šæª”æ¡ˆ '{filename}' ä¸å­˜åœ¨æ–¼ç•¶å‰ç›®éŒ„ '{os.getcwd()}'ã€‚")
            return []
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                markdown_content = f.read()
        except Exception as e:
            print(f"âŒ è®€å–æª”æ¡ˆ '{filename}' æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            return []

        parts = re.split(r'(## å·¥ä½œè¡¨:.*)', markdown_content)
        sections = []
        for i in range(1, len(parts), 2):
            if i + 1 < len(parts):
                title = parts[i].strip()
                content = parts[i + 1].strip()
                if title and content:
                    sections.append({"title": title, "content": content})
        
        if not sections:
            print(f"âš ï¸ è­¦å‘Šï¼šæœªèƒ½å¾æª”æ¡ˆ '{filename}' è§£æå‡ºä»»ä½•å·¥ä½œè¡¨å€å¡Šã€‚")
        else:
            print(f"å¾ '{filename}' è§£æå‡º {len(sections)} å€‹å€å¡Šã€‚")
        return sections

    def _filter_sections_by_title(self, all_sections):
        """æ ¹æ“šæ¨™é¡Œéæ¿¾å€å¡Šã€‚"""
        allowed_identifiers = self.config["ALLOWED_WORKSHEET_IDENTIFIERS"]
        return [sec for sec in all_sections if
                any(allowed_id in sec.get("title", "") for allowed_id in allowed_identifiers)]

    def _extract_keywords_rule_based(self, user_input):
        """ä½¿ç”¨è¦å‰‡æå–é—œéµå­—ã€‚"""
        # ... (æ­¤è™•çœç•¥æ‚¨åŸæœ¬çš„ç¨‹å¼ç¢¼ä»¥ä¿æŒç°¡æ½”ï¼ŒåŠŸèƒ½ä¸è®Š) ...
        print(f"--- (éšæ®µ0) ä½¿ç”¨è¦å‰‡è§£æè¼¸å…¥ (ä¸»è¦æå–åŸæ–™): '{user_input}' ---")
        tokens = list(jieba.cut_for_search(user_input.strip().lower()))
        potential_materials = []
        identified_characteristics = set()
        for token in tokens:
            token_clean = token.strip()
            if not token_clean or token_clean in self.config["CHINESE_STOP_WORDS"]: continue
            is_characteristic = False
            for target_char in self.config["TARGET_DESCRIPTION_KEYWORDS"]:
                if token_clean == target_char.lower():
                    identified_characteristics.add(target_char)
                    is_characteristic = True
                    break
            if not is_characteristic and not token_clean.isnumeric() and len(token_clean) > 0:
                potential_materials.append(token_clean)
        
        if not potential_materials: return None
        return {"åŸæ–™åç¨±": sorted(list(set(potential_materials))), "ç‰¹æ€§æè¿°": sorted(list(identified_characteristics))}

    def _search_sections(self, keywords_data):
        """åˆæ­¥ç¯©é¸åŒ…å«é—œéµå­—çš„å·¥ä½œè¡¨ã€‚"""
        # ... (æ­¤è™•çœç•¥æ‚¨åŸæœ¬çš„ç¨‹å¼ç¢¼ä»¥ä¿æŒç°¡æ½”ï¼ŒåŠŸèƒ½ä¸è®Š) ...
        material_keywords = keywords_data.get("åŸæ–™åç¨±", [])
        if not material_keywords: return []
        relevant_sections = []
        for section in self.sections_to_search:
            text_to_search = section.get("title", "") + section.get("content", "")
            if any(keyword.lower() in text_to_search.lower() for keyword in material_keywords):
                relevant_sections.append(section)
        return relevant_sections

    async def _extract_relevant_text_async(self, section, keywords_data):
        """(ç¬¬ä¸€éšæ®µ LLM - éåŒæ­¥) æå–èˆ‡åŸæ–™æœ€ç›´æ¥ç›¸é—œçš„æ–‡å­—ç‰‡æ®µã€‚"""
        material_name_str = "ã€".join(keywords_data.get('åŸæ–™åç¨±', []))
        description_keywords_str = ', '.join(keywords_data.get('ç‰¹æ€§æè¿°', []))

        # --- ä½¿ç”¨äº†å„ªåŒ–å¾Œçš„å¼·åŒ–ç‰ˆ Prompt ---
        prompt_template_str = """
        ä½ çš„èº«ä»½æ˜¯ä¸€å€‹è‡ªå‹•åŒ–çš„ã€æ²’æœ‰æ„Ÿæƒ…çš„æ–‡å­—æå–æ©Ÿå™¨äººã€‚
        ä½ çš„å”¯ä¸€ä»»å‹™æ˜¯ï¼šåœ¨ä¸‹æ–¹æä¾›çš„ã€Œå·¥ä½œè¡¨å…§å®¹ã€ä¸­ï¼Œåƒ…æ‰¾å‡ºèˆ‡ã€Œä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ã€æœ€ç›´æ¥ç›¸é—œçš„ã€ä¸€å€‹æˆ–å¤šå€‹ç°¡çŸ­æ–‡å­—ç‰‡æ®µã€å¥å­æˆ–åˆ—è¡¨é …ã€‘ã€‚

        ä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ï¼šã€{material_name_str}ã€‘
        (ä½¿ç”¨è€…åŒæ™‚æåŠçš„ç›¸é—œè©å½™ï¼Œåƒ…ä¾›ä½ ç†è§£ä¸Šä¸‹æ–‡ï¼Œä¸ç”¨æ–¼æå–ï¼š{description_keywords_str})

        å·¥ä½œè¡¨å…§å®¹ï¼š
        ```markdown
        {text}
        ```

        ---
        **åš´æ ¼è¼¸å‡ºè¦å‰‡ (ABSOLUTE RULES):**

        1.  **ç²¾ç¢ºæå–**: åªè¼¸å‡ºåŒ…å«ã€Œä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ã€çš„å¥å­ã€æ“ä½œæ­¥é©Ÿæˆ–å…¶éå¸¸ç·Šå¯†çš„ä¸Šä¸‹æ–‡ã€‚ç¯„åœè¶Šå°è¶Šå¥½ã€‚
        2.  **ã€ç›´æ¥è¼¸å‡ºåŸæ–‡ã€‘**: ä½ çš„è¼¸å‡º**å¿…é ˆ**ç›´æ¥å°±æ˜¯å¾ã€Œå·¥ä½œè¡¨å…§å®¹ã€ä¸­è¤‡è£½å‡ºä¾†çš„æ–‡å­—ï¼Œä¸€å­—ä¸æ”¹ã€‚
        3.  **ã€åš´æ ¼ç¦æ­¢ã€‘æ·»åŠ ä»»ä½•é¡å¤–æ–‡å­—**: ä½ çš„è¼¸å‡ºä¸­ï¼Œ**çµ•å°ä¸å…è¨±**åŒ…å«ä»»ä½•ä½ è‡ªå·±å‰µé€ çš„ã€è§£é‡‹æ€§çš„ã€ç¸½çµæ€§çš„æ–‡å­—ã€‚
            -   **éŒ¯èª¤ç¯„ä¾‹ (ç¦æ­¢è¼¸å‡º)**: "æ ¹æ“šæ–‡ä»¶ï¼Œé£Ÿé¹½åœ¨ç¬¬4é»æåˆ°..."
            -   **éŒ¯èª¤ç¯„ä¾‹ (ç¦æ­¢è¼¸å‡º)**: "ä»¥ä¸‹æ˜¯æ‰¾åˆ°çš„ç›¸é—œå…§å®¹ï¼š"
            -   **éŒ¯èª¤ç¯„ä¾‹ (ç¦æ­¢è¼¸å‡º)**: "å¥½çš„ï¼Œé€™æ˜¯é—œæ–¼é£Ÿé¹½çš„è³‡è¨Šã€‚"
        4.  **ã€åš´æ ¼ç¦æ­¢ã€‘æå–å…ƒä¿¡æ¯**: çµ•å°ç¦æ­¢åŒ…å« '## å·¥ä½œè¡¨: ...' é€™ç¨®æ¨™é¡Œï¼Œæˆ–ä»»ä½• 'è£½è¡¨æ—¥æœŸ', 'è£½è¡¨äºº' ç­‰é è…³è³‡è¨Šã€‚
        5.  **æ‰¾ä¸åˆ°å…§å®¹çš„è™•ç†**: å¦‚æœåœ¨ã€Œå·¥ä½œè¡¨å…§å®¹ã€ä¸­æ‰¾ä¸åˆ°ä»»ä½•èˆ‡ã€Œä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ã€ç›´æ¥ç›¸é—œçš„å…§å®¹ï¼Œä½ çš„å”¯ä¸€è¼¸å‡º**å¿…é ˆ**æ˜¯ä»¥ä¸‹é€™æ®µå›ºå®šçš„æ–‡å­—ï¼Œä¸å¾—æœ‰ä»»ä½•å¢æ¸›ï¼š`NO_DIRECT_CONTENT_FOUND`
        6.  **è¼¸å‡ºæ ¼å¼**: ç›´æ¥è¼¸å‡ºæ–‡å­—å³å¯ï¼Œä¸è¦ä½¿ç”¨ markdown çš„ ` ``` ` å€å¡ŠåŒ…åœã€‚

        å†æ¬¡å¼·èª¿ï¼šä½ çš„ä»»å‹™æ˜¯è¤‡è£½è²¼ä¸Šï¼Œä¸æ˜¯ç¸½çµæˆ–è§£é‡‹ã€‚ç›´æ¥é–‹å§‹è¼¸å‡ºä½ æ‰¾åˆ°çš„åŸæ–‡ç‰‡æ®µã€‚
        """
        
        prompt_template = ChatPromptTemplate.from_template(prompt_template_str)
        chain = prompt_template | self.llm | StrOutputParser()
        
        print(f"  (Async) æ­£åœ¨è™•ç†å€å¡Š: {section['title']}...")
        try:
            # ä½¿ç”¨ ainvoke é€²è¡ŒéåŒæ­¥å‘¼å«
            relevant_text = await chain.ainvoke({
                "material_name_str": material_name_str,
                "description_keywords_str": description_keywords_str,
                "text": section["content"]
            })
            
            relevant_text = relevant_text.strip()
            is_found = "NO_DIRECT_CONTENT_FOUND" not in relevant_text and relevant_text
            
            if not is_found:
                 print(f" Â  Â  â†³ åœ¨å€å¡Š '{section['title']}' ä¸­æœªæ‰¾åˆ°å…§å®¹ã€‚")
            else:
                 print(f" Â  Â  â†³ å¾ '{section['title']}' æå–åˆ°å…§å®¹ã€‚")

            return {"title": section['title'], "text": relevant_text, "found": is_found}
        except Exception as e:
            print(f"âŒ å¾å€å¡Š '{section['title']}' éåŒæ­¥æå–æ™‚å‡ºéŒ¯: {e}")
            return {"title": section['title'], "text": "LLM æå–å¤±æ•—", "found": False}

    def _synthesize_results(self, keywords_data, extracted_texts):
        """(ç¬¬äºŒéšæ®µ LLM) å°‡æå–çš„æ–‡å­—ç‰‡æ®µæ•´åˆæˆçµ±ä¸€æ ¼å¼åˆ—è¡¨ã€‚"""
        # ... (æ­¤è™•çœç•¥æ‚¨åŸæœ¬çš„ç¨‹å¼ç¢¼ä»¥ä¿æŒç°¡æ½”ï¼ŒåŠŸèƒ½ä¸è®Šï¼Œä½†ç¾åœ¨æ¥æ”¶çš„æ˜¯ä¹¾æ·¨çš„è¼¸å…¥) ...
        valid_extractions = [item['text'] for item in extracted_texts if item.get("found")]
        if not valid_extractions:
            material_name_str = "ã€".join(keywords_data.get('åŸæ–™åç¨±', ["æ‰€æŸ¥è©¢çš„é …ç›®"]))
            return f"å·²æª¢æŸ¥æ‰€æœ‰ç›¸é—œSOPæ–‡ä»¶å€å¡Šï¼Œä½†å‡æœªæ‰¾åˆ°é—œæ–¼åŸæ–™ã€{material_name_str}ã€‘çš„ç›´æ¥æ“ä½œèªªæ˜æˆ–æ³¨æ„äº‹é …ã€‚"
        
        print(f"\nğŸ”„ (éšæ®µ2) æ­£åœ¨æ•´åˆ {len(valid_extractions)} ä»½æå–çš„é‡é»å…§å®¹...")
        combined_extracted_text = "\n\n---\n\n".join(valid_extractions)
        material_name = "ã€".join(keywords_data.get('åŸæ–™åç¨±', []))
        characteristics_list = keywords_data.get('ç‰¹æ€§æè¿°', [])

        synthesis_prompt_template_str = """
        æ‚¨æ˜¯ä¸€ä½SOPå…§å®¹æ•´ç†å“¡ã€‚æ‚¨çš„ä»»å‹™æ˜¯å°‡ä¸‹æ–¹æä¾›çš„ã€å·²å¾SOPæ–‡ä»¶ä¸­æå–å‡ºçš„ã€èˆ‡æŒ‡å®šåŸæ–™ç›¸é—œçš„ã€å¤šå€‹ç¨ç«‹çš„ç°¡çŸ­æ–‡å­—ç‰‡æ®µã€‘ï¼Œæ•´ç†æˆä¸€å€‹ã€æ¥µç°¡çš„ã€çµ±ä¸€æ ¼å¼çš„æ•¸å­—ç·¨è™Ÿåˆ—è¡¨ã€‘ã€‚
        ä½¿ç”¨è€…ä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ç‚ºã€{material_name}ã€‘ã€‚(ä½¿ç”¨è€…æŸ¥è©¢æ™‚æåŠçš„ç›¸é—œè©å½™ï¼Œä¾›æ‚¨ç†è§£ä¸Šä¸‹æ–‡ï¼š{characteristics_list})
        
        å·²æå–çš„ç›¸é—œSOPç‰‡æ®µ (è«‹å°‡å®ƒå€‘è¦–ç‚ºç¨ç«‹çš„è³‡è¨Šé»)ï¼š
        ---
        {combined_extracted_text}
        ---

        æ‚¨çš„ä»»å‹™èˆ‡è¼¸å‡ºè¦æ±‚ï¼š
        1.  **ã€æ ¸å¿ƒä»»å‹™ã€‘ï¼š** å°‡é€™äº›ç‰‡æ®µä¸­çš„ã€æ¯ä¸€å€‹ç¨ç«‹çš„è³‡è¨Šé»ã€æ“ä½œæ­¥é©Ÿã€æˆ–æ³¨æ„äº‹é …ã€‘æ•´ç†å‡ºä¾†ï¼Œä½œç‚ºåˆ—è¡¨ä¸­çš„ä¸€å€‹ç¨ç«‹é …ç›®ã€‚
        2.  **ã€æ ¼å¼çµ±ä¸€ã€‘ï¼š** ä½¿ç”¨å¾ 1. é–‹å§‹çš„æ•¸å­—ç·¨è™Ÿåˆ—è¡¨ã€‚
        3.  **ã€åŸæ–‡å‘ˆç¾ã€‘ï¼š** ç›¡æœ€å¤§å¯èƒ½ã€ç›´æ¥ä½¿ç”¨ã€‘æå–ç‰‡æ®µä¸­çš„ã€åŸæ–‡è¡¨è¿°ã€‘ã€‚**ã€åš´æ ¼ç¦æ­¢ã€‘** ä»»ä½•å½¢å¼çš„æ”¹å¯«ã€æ‘˜è¦ã€è§£é‡‹æˆ–æ­¸ç´ã€‚
        4.  **ã€æ¥µç°¡è¼¸å‡ºã€‘ï¼š** æ‚¨çš„æœ€çµ‚è¼¸å‡ºã€å¿…é ˆç›´æ¥æ˜¯é€™å€‹æ•¸å­—ç·¨è™Ÿåˆ—è¡¨æœ¬èº«ã€‘ã€‚**ã€åš´æ ¼ç¦æ­¢ã€‘** åŒ…å«ä»»ä½•å‰è¨€ã€æ¨™é¡Œæˆ–çµèªã€‚
        5.  å¦‚æœå¤šå€‹ç‰‡æ®µè³‡è¨Šé‡è¤‡ï¼Œè«‹åªä¿ç•™ä¸€å€‹æœ€æ¸…æ™°çš„ã€‚
        6.  ä½¿ç”¨**ç¹é«”ä¸­æ–‡**ã€‚
        è«‹ç›´æ¥é–‹å§‹è¼¸å‡ºåˆ—è¡¨ï¼š
        """
        
        synthesis_prompt = ChatPromptTemplate.from_template(synthesis_prompt_template_str)
        synthesis_chain = synthesis_prompt | self.llm | StrOutputParser()
        
        final_response = synthesis_chain.invoke({
            "material_name": material_name,
            "characteristics_list": ', '.join(characteristics_list),
            "combined_extracted_text": combined_extracted_text
        })
        
        # ... (æ‚¨åŸæœ¬çš„å¾Œè™•ç†é‚è¼¯å¯ä»¥ä¿ç•™æˆ–ç°¡åŒ–) ...
        return final_response.strip()


    async def process_query(self, user_query):
        """è™•ç†å–®ä¸€ä½¿ç”¨è€…æŸ¥è©¢ä¸¦è¿”å›çµæœ (éåŒæ­¥)ã€‚"""
        if not self.initialization_success:
            return "ç³»çµ±åˆå§‹åŒ–å¤±æ•—ï¼Œç„¡æ³•è™•ç†æŸ¥è©¢ã€‚"

        print(f"\nè™•ç†æŸ¥è©¢: '{user_query}'")
        start_time = time.time()

        try:
            keywords_data = self._extract_keywords_rule_based(user_query)
            if not keywords_data or not keywords_data.get("åŸæ–™åç¨±"):
                return "ç„¡æ³•å¾æ‚¨çš„è¨Šæ¯ä¸­è§£æå‡ºæœ‰æ•ˆçš„åŸæ–™åç¨±é€²è¡ŒæŸ¥è©¢ã€‚"

            relevant_sop_sections = self._search_sections(keywords_data)
            if not relevant_sop_sections:
                material_name_str = "ã€".join(keywords_data.get("åŸæ–™åç¨±", ["æœªçŸ¥åŸæ–™"]))
                return f"åœ¨SOPæ–‡ä»¶ä¸­ï¼Œæ‰¾ä¸åˆ°èˆ‡åŸæ–™ã€{material_name_str}ã€‘ç›´æ¥ç›¸é—œçš„å·¥ä½œè¡¨ã€‚"

            # ä½¿ç”¨ asyncio.gather ä¸¦è¡ŒåŸ·è¡Œæ‰€æœ‰æå–ä»»å‹™
            tasks = [self._extract_relevant_text_async(section, keywords_data) for section in relevant_sop_sections]
            extracted_texts = await asyncio.gather(*tasks)

            final_summary = self._synthesize_results(keywords_data, extracted_texts)
            reply_text = final_summary

        except Exception as e:
            print(f"!!!!!!!!!! è™•ç†æŸ¥è©¢ '{user_query}' æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤ !!!!!!!!!!")
            traceback.print_exc()
            reply_text = f"è™•ç†æŸ¥è©¢æ™‚é‡åˆ°æœªé æœŸçš„éŒ¯èª¤ï¼Œè«‹æª¢æŸ¥æ—¥èªŒã€‚"

        end_time = time.time()
        print(f"æŸ¥è©¢ \"{user_query}\" è™•ç†å®Œæˆï¼Œè€—æ™‚ {end_time - start_time:.2f} ç§’ã€‚")
        return reply_text if reply_text.strip() else "æŠ±æ­‰ï¼Œæœªèƒ½æ‰¾åˆ°æ˜ç¢ºçš„è³‡è¨Šã€‚"

# --- ä¸»åŸ·è¡Œå€å¡Š ---
async def main():
    """ç¨‹å¼é€²å…¥é»ï¼ŒåŸ·è¡ŒéåŒæ­¥çš„æŸ¥è©¢è¿´åœˆã€‚"""
    sop_system = SOPQuerySystem()

    if sop_system.initialization_success:
        print("\n--- ç³»çµ±å·²å°±ç·’ï¼Œè«‹è¼¸å…¥æ‚¨çš„æŸ¥è©¢ ---")
        print("    (ä¾‹å¦‚ï¼š'é£Ÿé¹½ çµå¡Š')")
        print("    (è¼¸å…¥ 'exit' æˆ– 'quit' ä¾†çµæŸç¨‹å¼)")
        
        while True:
            try:
                user_input = await asyncio.to_thread(input, "\næ‚¨çš„æŸ¥è©¢: ")
                if user_input.strip().lower() in ['exit', 'quit']:
                    print("æ­£åœ¨çµæŸç¨‹å¼...")
                    break
                if not user_input.strip():
                    continue

                result = await sop_system.process_query(user_input)
                print("\n========== æŸ¥è©¢çµæœ ==========")
                print(result)
                print("==============================")
            except (KeyboardInterrupt, EOFError):
                print("\nåµæ¸¬åˆ°ä½¿ç”¨è€…ä¸­æ–·ï¼Œæ­£åœ¨çµæŸç¨‹å¼...")
                break
            except Exception as e:
                print(f"\nåœ¨ä¸»æŸ¥è©¢è¿´åœˆä¸­ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")
                traceback.print_exc()
    else:
        print("\nâŒ å› ç³»çµ±åˆå§‹åŒ–å¤±æ•—ï¼Œç„¡æ³•å•Ÿå‹• SOP æŸ¥è©¢ç³»çµ±ã€‚è«‹æª¢æŸ¥ä¸Šæ–¹çš„éŒ¯èª¤è¨Šæ¯ã€‚")
    
    print("--- ç¨‹å¼åŸ·è¡Œå®Œç•¢ ---")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nç¨‹å¼è¢«å¼·åˆ¶çµ‚æ­¢ã€‚")