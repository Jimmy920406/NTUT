import os
import re
import traceback
import time
import asyncio
import sys

# --- å¿…è¦çš„å¥—ä»¶å¼•å…¥ ---
from dotenv import load_dotenv
import jieba
# ä¿®æ”¹é» 1: å°‡ Groq çš„å¼•å…¥æ›æˆ OpenAI
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser


class SOPQuerySystem:
    """
    å°‡æ•´å€‹ SOP æŸ¥è©¢æµç¨‹å°è£åœ¨ä¸€å€‹é¡åˆ¥ä¸­ï¼Œæ–¹ä¾¿ç®¡ç†ç‹€æ…‹èˆ‡è¨­å®šã€‚
    """
    def __init__(self):
        """åˆå§‹åŒ–ç³»çµ±ï¼Œè¼‰å…¥è¨­å®šã€LLM å’Œæ–‡ä»¶ã€‚"""
        print("--- é–‹å§‹åˆå§‹åŒ– SOP æŸ¥è©¢ç³»çµ± (ä½¿ç”¨ OpenAI) ---")
        self._load_config()
        self.llm = None
        self.sections_to_search = []
        self.initialization_success = self._initialize()

    def _load_config(self):
        """è¼‰å…¥æ‰€æœ‰è¨­å®šæª”"""
        load_dotenv()
        self.config = {
            # ä¿®æ”¹é» 2: æ›´æ–°é è¨­æ¨¡å‹åç¨±å’Œ API Key çš„è®€å–
            "MODEL_NAME": os.getenv("MODEL_NAME", "gpt-4o-mini"),
            "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY"),
            "SIMPLIFIED_MD_FILENAME": os.getenv("SIMPLIFIED_MD_FILENAME", "simplified_output_by_section.md"),
            "TARGET_DESCRIPTION_KEYWORDS": ["çµå¡Š", "éç¯©", "é †åº", "å¸æ¿•", "ç¨ åº¦", "é»ç¨ ", "æµå‹•æ€§"],
            "CHINESE_STOP_WORDS": {"çš„", "å’Œ", "èˆ‡", "æˆ–", "äº†", "å‘¢", "å—", "å–”", "å•Š", "é—œæ–¼", "æœ‰é—œ", "è«‹", "è«‹å•", " ", ""},
            "ALLOWED_WORKSHEET_IDENTIFIERS": ["å·¥ä½œè¡¨: 9", "å·¥ä½œè¡¨: 10"]
        }

    def _initialize(self):
        """åŸ·è¡Œåˆå§‹åŒ–æ­¥é©Ÿï¼šè¨­å®š LLM å’Œè¼‰å…¥æ–‡ä»¶ã€‚"""
        # ä¿®æ”¹é» 3: å®Œå…¨æ›¿æ›ç‚º OpenAI çš„åˆå§‹åŒ–é‚è¼¯
        # 1. åˆå§‹åŒ– LangChain çš„ ChatOpenAI ç‰©ä»¶
        if not self.config["OPENAI_API_KEY"] or not self.config["MODEL_NAME"]:
            print("âŒ éŒ¯èª¤ï¼šæœªèƒ½ç²å– OPENAI_API_KEY æˆ– MODEL_NAMEï¼Œç„¡æ³•åˆå§‹åŒ– ChatOpenAIã€‚")
            return False
        try:
            self.llm = ChatOpenAI(model=self.config["MODEL_NAME"], openai_api_key=self.config["OPENAI_API_KEY"])
            print(f"âœ… ChatOpenAI (LangChain) for model '{self.config['MODEL_NAME']}' åˆå§‹åŒ–æˆåŠŸã€‚")
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ– ChatOpenAI æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            return False

        # 2. è¼‰å…¥ä¸¦éæ¿¾ SOP æ–‡ä»¶å€å¡Š (é€™éƒ¨åˆ†é‚è¼¯ä¸è®Š)
        all_sections = self._load_markdown_sections()
        if not all_sections:
            print(f"âŒ éŒ¯èª¤ï¼šæœªèƒ½å¾ '{self.config['SIMPLIFIED_MD_FILENAME']}' è¼‰å…¥ä»»ä½• SOP æ–‡ä»¶å€å¡Šã€‚")
            return False

        self.sections_to_search = self._filter_sections_by_title(all_sections)
        if not self.sections_to_search:
            print(f"âš ï¸ è­¦å‘Šï¼šæœªéæ¿¾å‡ºä»»ä½•ç›®æ¨™å€å¡Šï¼Œå°‡åœ¨å…¨éƒ¨ {len(all_sections)} å€‹å€å¡Šä¸­æœå°‹ã€‚")
            self.sections_to_search = all_sections
        
        print(f"âœ… æˆåŠŸæº–å‚™ {len(self.sections_to_search)} å€‹å€å¡Šä¾›æŸ¥è©¢ã€‚")
        return True

    def _load_markdown_sections(self):
        """å¾æª”æ¡ˆè®€å–ä¸¦è§£æ Markdown å€å¡Šã€‚"""
        # é€™éƒ¨åˆ†çš„ç¨‹å¼ç¢¼å®Œå…¨ä¸éœ€è¦ä¿®æ”¹
        script_dir = os.path.dirname(__file__)
        filename = os.path.abspath(os.path.join(script_dir, '..', self.config["SIMPLIFIED_MD_FILENAME"]))
        
        print(f"(SUT) æ­£åœ¨å¾çµ•å°è·¯å¾‘è¼‰å…¥æª”æ¡ˆ: {filename}")
        if not os.path.exists(filename):
            print(f"âŒ (SUT) éŒ¯èª¤ï¼šæª”æ¡ˆ '{filename}' ä¸å­˜åœ¨ã€‚")
            return []
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                markdown_content = f.read()
        except Exception as e:
            print(f"âŒ (SUT) è®€å–æª”æ¡ˆ '{filename}' æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            return []

        parts = re.split(r'(## å·¥ä½œè¡¨:.*)', markdown_content)
        sections = []
        for i in range(1, len(parts), 2):
            if i + 1 < len(parts):
                title = parts[i].strip()
                content = parts[i + 1].strip()
                if title and content:
                    sections.append({"title": title, "content": content})
        
        if not sections:
            print(f"âš ï¸ (SUT) è­¦å‘Šï¼šæœªèƒ½å¾æª”æ¡ˆ '{filename}' è§£æå‡ºä»»ä½•å·¥ä½œè¡¨å€å¡Šã€‚")
        else:
            print(f"(SUT) å¾ '{filename}' è§£æå‡º {len(sections)} å€‹å€å¡Šã€‚")
        return sections

    def _filter_sections_by_title(self, all_sections):
        """æ ¹æ“šæ¨™é¡Œéæ¿¾å€å¡Šã€‚"""
        # é€™éƒ¨åˆ†çš„ç¨‹å¼ç¢¼å®Œå…¨ä¸éœ€è¦ä¿®æ”¹
        allowed_identifiers = self.config["ALLOWED_WORKSHEET_IDENTIFIERS"]
        return [sec for sec in all_sections if
                any(allowed_id in sec.get("title", "") for allowed_id in allowed_identifiers)]

    # --- ä»¥ä¸‹æ‰€æœ‰ RAG æµç¨‹çš„å‡½å¼ï¼Œéƒ½å› ç‚º LangChain çš„æŠ½è±¡åŒ–è€Œã€å®Œå…¨ä¸éœ€è¦ä¿®æ”¹ã€‘ ---

    def _extract_keywords_rule_based(self, user_input):
        """ä½¿ç”¨è¦å‰‡æå–é—œéµå­—ã€‚"""
        # ... (æ­¤è™•çœç•¥ä»¥ä¿æŒç°¡æ½”ï¼Œæ‚¨çš„ç¨‹å¼ç¢¼ä¸éœ€è®Šå‹•) ...
        print(f"--- (éšæ®µ0) ä½¿ç”¨è¦å‰‡è§£æè¼¸å…¥ (ä¸»è¦æå–åŸæ–™): '{user_input}' ---")
        tokens = list(jieba.cut_for_search(user_input.strip().lower()))
        potential_materials = []
        identified_characteristics = set()
        for token in tokens:
            token_clean = token.strip()
            if not token_clean or token_clean in self.config["CHINESE_STOP_WORDS"]: continue
            is_characteristic = False
            for target_char in self.config["TARGET_DESCRIPTION_KEYWORDS"]:
                if token_clean == target_char.lower():
                    identified_characteristics.add(target_char)
                    is_characteristic = True
                    break
            if not is_characteristic and not token_clean.isnumeric() and len(token_clean) > 0:
                potential_materials.append(token_clean)
        
        if not potential_materials: return None
        return {"åŸæ–™åç¨±": sorted(list(set(potential_materials))), "ç‰¹æ€§æè¿°": sorted(list(identified_characteristics))}

    def _search_sections(self, keywords_data):
        """åˆæ­¥ç¯©é¸åŒ…å«é—œéµå­—çš„å·¥ä½œè¡¨ã€‚"""
        # ... (æ­¤è™•çœç•¥ä»¥ä¿æŒç°¡æ½”ï¼Œæ‚¨çš„ç¨‹å¼ç¢¼ä¸éœ€è®Šå‹•) ...
        material_keywords = keywords_data.get("åŸæ–™åç¨±", [])
        if not material_keywords: return []
        relevant_sections = []
        for section in self.sections_to_search:
            text_to_search = section.get("title", "") + section.get("content", "")
            if any(keyword.lower() in text_to_search.lower() for keyword in material_keywords):
                relevant_sections.append(section)
        return relevant_sections
        
    async def _extract_relevant_text_async(self, section, keywords_data):
        """(ç¬¬ä¸€éšæ®µ LLM - éåŒæ­¥) æå–èˆ‡åŸæ–™æœ€ç›´æ¥ç›¸é—œçš„æ–‡å­—ç‰‡æ®µã€‚"""
        # ... (æ­¤è™•çœç•¥ä»¥ä¿æŒç°¡æ½”ï¼Œæ‚¨çš„ç¨‹å¼ç¢¼ä¸éœ€è®Šå‹•) ...
        material_name_str = "ã€".join(keywords_data.get('åŸæ–™åç¨±', []))
        description_keywords_str = ', '.join(keywords_data.get('ç‰¹æ€§æè¿°', []))
        prompt_template_str = """
        ä½ çš„èº«ä»½æ˜¯ä¸€å€‹è‡ªå‹•åŒ–çš„ã€æ²’æœ‰æ„Ÿæƒ…çš„æ–‡å­—æå–æ©Ÿå™¨äººã€‚
        ä½ çš„å”¯ä¸€ä»»å‹™æ˜¯ï¼šåœ¨ä¸‹æ–¹æä¾›çš„ã€Œå·¥ä½œè¡¨å…§å®¹ã€ä¸­ï¼Œåƒ…æ‰¾å‡ºèˆ‡ã€Œä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ã€æœ€ç›´æ¥ç›¸é—œçš„ã€ä¸€å€‹æˆ–å¤šå€‹ç°¡çŸ­æ–‡å­—ç‰‡æ®µã€å¥å­æˆ–åˆ—è¡¨é …ã€‘ã€‚

        ä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ï¼šã€{material_name_str}ã€‘
        (ä½¿ç”¨è€…åŒæ™‚æåŠçš„ç›¸é—œè©å½™ï¼Œåƒ…ä¾›ä½ ç†è§£ä¸Šä¸‹æ–‡ï¼Œä¸ç”¨æ–¼æå–ï¼š{description_keywords_str})

        å·¥ä½œè¡¨å…§å®¹ï¼š
        ```markdown
        {text}
        ```
        ---
        **åš´æ ¼è¼¸å‡ºè¦å‰‡ (ABSOLUTE RULES):**
        1.  **ç²¾ç¢ºæå–**: åªè¼¸å‡ºåŒ…å«ã€Œä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ã€çš„å¥å­ã€æ“ä½œæ­¥é©Ÿæˆ–å…¶éå¸¸ç·Šå¯†çš„ä¸Šä¸‹æ–‡ã€‚ç¯„åœè¶Šå°è¶Šå¥½ã€‚
        2.  **ã€ç›´æ¥è¼¸å‡ºåŸæ–‡ã€‘**: ä½ çš„è¼¸å‡º**å¿…é ˆ**ç›´æ¥å°±æ˜¯å¾ã€Œå·¥ä½œè¡¨å…§å®¹ã€ä¸­è¤‡è£½å‡ºä¾†çš„æ–‡å­—ï¼Œä¸€å­—ä¸æ”¹ã€‚
        3.  **ã€åš´æ ¼ç¦æ­¢ã€‘æ·»åŠ ä»»ä½•é¡å¤–æ–‡å­—**
        4.  **ã€åš´æ ¼ç¦æ­¢ã€‘æå–å…ƒä¿¡æ¯**
        5.  **æ‰¾ä¸åˆ°å…§å®¹çš„è™•ç†**: å¦‚æœæ‰¾ä¸åˆ°ï¼Œå”¯ä¸€è¼¸å‡º**å¿…é ˆ**æ˜¯ï¼š`NO_DIRECT_CONTENT_FOUND`
        6.  **è¼¸å‡ºæ ¼å¼**: ç›´æ¥è¼¸å‡ºæ–‡å­—å³å¯ï¼Œä¸è¦ä½¿ç”¨ markdown çš„ ` ``` ` å€å¡ŠåŒ…åœã€‚
        """
        prompt_template = ChatPromptTemplate.from_template(prompt_template_str)
        chain = prompt_template | self.llm | StrOutputParser()
        print(f" Â (Async) æ­£åœ¨è™•ç†å€å¡Š: {section['title']}...")
        try:
            relevant_text = await chain.ainvoke({"material_name_str": material_name_str, "description_keywords_str": description_keywords_str, "text": section["content"]})
            relevant_text = relevant_text.strip()
            is_found = "NO_DIRECT_CONTENT_FOUND" not in relevant_text and relevant_text
            if not is_found: print(f" Â  Â  â†³ åœ¨å€å¡Š '{section['title']}' ä¸­æœªæ‰¾åˆ°å…§å®¹ã€‚")
            else: print(f" Â  Â  â†³ å¾ '{section['title']}' æå–åˆ°å…§å®¹ã€‚")
            return {"title": section['title'], "text": relevant_text, "found": is_found}
        except Exception as e:
            print(f"âŒ å¾å€å¡Š '{section['title']}' éåŒæ­¥æå–æ™‚å‡ºéŒ¯: {e}")
            return {"title": section['title'], "text": "LLM æå–å¤±æ•—", "found": False}

    def _synthesize_results(self, keywords_data, extracted_texts):
        """(ç¬¬äºŒéšæ®µ LLM) å°‡æå–çš„æ–‡å­—ç‰‡æ®µæ•´åˆæˆçµ±ä¸€æ ¼å¼åˆ—è¡¨ã€‚"""
        # ... (æ­¤è™•çœç•¥ä»¥ä¿æŒç°¡æ½”ï¼Œæ‚¨çš„ç¨‹å¼ç¢¼ä¸éœ€è®Šå‹•) ...
        valid_extractions = [item['text'] for item in extracted_texts if item.get("found")]
        if not valid_extractions:
            material_name_str = "ã€".join(keywords_data.get('åŸæ–™åç¨±', ["æ‰€æŸ¥è©¢çš„é …ç›®"]))
            return f"å·²æª¢æŸ¥æ‰€æœ‰ç›¸é—œSOPæ–‡ä»¶å€å¡Šï¼Œä½†å‡æœªæ‰¾åˆ°é—œæ–¼åŸæ–™ã€{material_name_str}ã€‘çš„ç›´æ¥æ“ä½œèªªæ˜æˆ–æ³¨æ„äº‹é …ã€‚"
        print(f"\nğŸ”„ (éšæ®µ2) æ­£åœ¨æ•´åˆ {len(valid_extractions)} ä»½æå–çš„é‡é»å…§å®¹...")
        combined_extracted_text = "\n\n---\n\n".join(valid_extractions)
        material_name = "ã€".join(keywords_data.get('åŸæ–™åç¨±', []))
        characteristics_list = keywords_data.get('ç‰¹æ€§æè¿°', [])
        synthesis_prompt_template_str = """
        æ‚¨æ˜¯ä¸€ä½SOPå…§å®¹æ•´ç†å“¡ã€‚æ‚¨çš„ä»»å‹™æ˜¯å°‡ä¸‹æ–¹æä¾›çš„ã€å·²å¾SOPæ–‡ä»¶ä¸­æå–å‡ºçš„ã€èˆ‡æŒ‡å®šåŸæ–™ç›¸é—œçš„ã€å¤šå€‹ç¨ç«‹çš„ç°¡çŸ­æ–‡å­—ç‰‡æ®µã€‘ï¼Œæ•´ç†æˆä¸€å€‹ã€æ¥µç°¡çš„ã€çµ±ä¸€æ ¼å¼çš„æ•¸å­—ç·¨è™Ÿåˆ—è¡¨ã€‘ã€‚
        ä½¿ç”¨è€…ä¸»è¦æŸ¥è©¢çš„åŸæ–™åç¨±ç‚ºã€{material_name}ã€‘ã€‚(ä½¿ç”¨è€…æŸ¥è©¢æ™‚æåŠçš„ç›¸é—œè©å½™ï¼Œä¾›æ‚¨ç†è§£ä¸Šä¸‹æ–‡ï¼š{characteristics_list})
        
        å·²æå–çš„ç›¸é—œSOPç‰‡æ®µ (è«‹å°‡å®ƒå€‘è¦–ç‚ºç¨ç«‹çš„è³‡è¨Šé»)ï¼š
        ---
        {combined_extracted_text}
        ---

        æ‚¨çš„ä»»å‹™èˆ‡è¼¸å‡ºè¦æ±‚ï¼š
        1.  **ã€æ ¸å¿ƒä»»å‹™ã€‘ï¼š** å°‡é€™äº›ç‰‡æ®µæ•´ç†æˆåˆ—è¡¨ä¸­çš„ä¸€å€‹ç¨ç«‹é …ç›®ã€‚
        2.  **ã€æ ¼å¼çµ±ä¸€ã€‘ï¼š** ä½¿ç”¨å¾ 1. é–‹å§‹çš„æ•¸å­—ç·¨è™Ÿåˆ—è¡¨ã€‚
        3.  **ã€åŸæ–‡å‘ˆç¾ã€‘ï¼š** ç›¡æœ€å¤§å¯èƒ½ã€ç›´æ¥ä½¿ç”¨ã€‘åŸæ–‡è¡¨è¿°ï¼Œã€åš´æ ¼ç¦æ­¢ã€‘ä»»ä½•å½¢å¼çš„æ”¹å¯«æˆ–æ‘˜è¦ã€‚
        4.  **ã€æ¥µç°¡è¼¸å‡ºã€‘ï¼š** æ‚¨çš„æœ€çµ‚è¼¸å‡ºã€å¿…é ˆç›´æ¥æ˜¯é€™å€‹æ•¸å­—ç·¨è™Ÿåˆ—è¡¨æœ¬èº«ã€‘ã€‚
        5.  å¦‚æœå¤šå€‹ç‰‡æ®µè³‡è¨Šé‡è¤‡ï¼Œè«‹åªä¿ç•™ä¸€å€‹ã€‚
        è«‹ç›´æ¥é–‹å§‹è¼¸å‡ºåˆ—è¡¨ï¼š
        """
        synthesis_prompt = ChatPromptTemplate.from_template(synthesis_prompt_template_str)
        synthesis_chain = synthesis_prompt | self.llm | StrOutputParser()
        final_response = synthesis_chain.invoke({"material_name": material_name, "characteristics_list": ', '.join(characteristics_list), "combined_extracted_text": combined_extracted_text})
        return final_response.strip()

    async def process_query(self, user_query):
        """è™•ç†å–®ä¸€ä½¿ç”¨è€…æŸ¥è©¢ä¸¦è¿”å›çµæœ (éåŒæ­¥)ã€‚"""
        # ... (æ­¤è™•çœç•¥ä»¥ä¿æŒç°¡æ½”ï¼Œæ‚¨çš„ç¨‹å¼ç¢¼ä¸éœ€è®Šå‹•) ...
        if not self.initialization_success:
            return "ç³»çµ±åˆå§‹åŒ–å¤±æ•—ï¼Œç„¡æ³•è™•ç†æŸ¥è©¢ã€‚"
        print(f"\nè™•ç†æŸ¥è©¢: '{user_query}'")
        start_time = time.time()
        try:
            keywords_data = self._extract_keywords_rule_based(user_query)
            if not keywords_data or not keywords_data.get("åŸæ–™åç¨±"):
                return "ç„¡æ³•å¾æ‚¨çš„è¨Šæ¯ä¸­è§£æå‡ºæœ‰æ•ˆçš„åŸæ–™åç¨±é€²è¡ŒæŸ¥è©¢ã€‚"
            relevant_sop_sections = self._search_sections(keywords_data)
            if not relevant_sop_sections:
                material_name_str = "ã€".join(keywords_data.get("åŸæ–™åç¨±", ["æœªçŸ¥åŸæ–™"]))
                return f"åœ¨SOPæ–‡ä»¶ä¸­ï¼Œæ‰¾ä¸åˆ°èˆ‡åŸæ–™ã€{material_name_str}ã€‘ç›´æ¥ç›¸é—œçš„å·¥ä½œè¡¨ã€‚"
            tasks = [self._extract_relevant_text_async(section, keywords_data) for section in relevant_sop_sections]
            extracted_texts = await asyncio.gather(*tasks)
            final_summary = self._synthesize_results(keywords_data, extracted_texts)
            reply_text = final_summary
        except Exception as e:
            print(f"!!!!!!!!!! è™•ç†æŸ¥è©¢ '{user_query}' æ™‚ç™¼ç”Ÿåš´é‡éŒ¯èª¤ !!!!!!!!!!")
            traceback.print_exc()
            reply_text = f"è™•ç†æŸ¥è©¢æ™‚é‡åˆ°æœªé æœŸçš„éŒ¯èª¤ï¼Œè«‹æª¢æŸ¥æ—¥èªŒã€‚"
        end_time = time.time()
        print(f"æŸ¥è©¢ \"{user_query}\" è™•ç†å®Œæˆï¼Œè€—æ™‚ {end_time - start_time:.2f} ç§’ã€‚")
        return reply_text if reply_text.strip() else "æŠ±æ­‰ï¼Œæœªèƒ½æ‰¾åˆ°æ˜ç¢ºçš„è³‡è¨Šã€‚"


# --- ä¸»åŸ·è¡Œå€å¡Š ---
async def main():
    """ç¨‹å¼é€²å…¥é»ï¼ŒåŸ·è¡ŒéåŒæ­¥çš„æŸ¥è©¢è¿´åœˆã€‚"""
    # é€™éƒ¨åˆ†çš„ç¨‹å¼ç¢¼å®Œå…¨ä¸éœ€è¦ä¿®æ”¹
    sop_system = SOPQuerySystem()

    if sop_system.initialization_success:
        print("\n--- ç³»çµ±å·²å°±ç·’ï¼Œè«‹è¼¸å…¥æ‚¨çš„æŸ¥è©¢ ---")
        print("    (ä¾‹å¦‚ï¼š'é£Ÿé¹½ çµå¡Š')")
        print("    (è¼¸å…¥ 'exit' æˆ– 'quit' ä¾†çµæŸç¨‹å¼)")
        
        while True:
            try:
                user_input = await asyncio.to_thread(input, "\næ‚¨çš„æŸ¥è©¢: ")
                if user_input.strip().lower() in ['exit', 'quit']:
                    print("æ­£åœ¨çµæŸç¨‹å¼...")
                    break
                if not user_input.strip():
                    continue

                result = await sop_system.process_query(user_input)
                print("\n========== æŸ¥è©¢çµæœ ==========")
                print(result)
                print("==============================")
            except (KeyboardInterrupt, EOFError):
                print("\nåµæ¸¬åˆ°ä½¿ç”¨è€…ä¸­æ–·ï¼Œæ­£åœ¨çµæŸç¨‹å¼...")
                break
            except Exception as e:
                print(f"\nåœ¨ä¸»æŸ¥è©¢è¿´åœˆä¸­ç™¼ç”Ÿæœªé æœŸéŒ¯èª¤: {e}")
                traceback.print_exc()
    else:
        print("\nâŒ å› ç³»çµ±åˆå§‹åŒ–å¤±æ•—ï¼Œç„¡æ³•å•Ÿå‹• SOP æŸ¥è©¢ç³»çµ±ã€‚è«‹æª¢æŸ¥ä¸Šæ–¹çš„éŒ¯èª¤è¨Šæ¯ã€‚")
    
    print("--- ç¨‹å¼åŸ·è¡Œå®Œç•¢ ---")

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nç¨‹å¼è¢«å¼·åˆ¶çµ‚æ­¢ã€‚")
