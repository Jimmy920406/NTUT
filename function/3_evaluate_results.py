import os
import json
import asyncio
from dotenv import load_dotenv

# --- å¿…è¦çš„å¥—ä»¶å¼•å…¥ ---
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import JsonOutputParser
from pydantic import BaseModel, Field, field_validator

# --- æ–°å¢çš„è¨­å®šï¼šæ§åˆ¶è©•ä¼°çš„æ‰¹æ¬¡å¤§å°å’Œå»¶é²æ™‚é–“ ---
BATCH_SIZE = 5  # æ¯æ‰¹è©•ä¼° 5 å€‹çµæœ
DELAY_BETWEEN_BATCHES = 0  # æ¯æ‰¹è™•ç†å®Œå¾Œï¼Œä¼‘æ¯ 10 ç§’

def initialize_llm():
    """è¼‰å…¥ç’°å¢ƒè®Šæ•¸ä¸¦åˆå§‹åŒ– OpenAI LLM ç‰©ä»¶ã€‚"""
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY")
    model_name = os.getenv("MODEL_NAME", "gpt-4o-mini")
    if not api_key:
        print("âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° OPENAI_API_KEYã€‚")
        return None
    try:
        # å°‡æº«åº¦è¨­ç‚º0ï¼ŒåŠ›æ±‚å®¢è§€
        llm = ChatOpenAI(model=model_name, openai_api_key=api_key)
        print(f"âœ… LLM ({model_name}) åˆå§‹åŒ–æˆåŠŸï¼Œç”¨æ–¼è©•ä¼°ã€‚")
        return llm
    except Exception as e:
        print(f"âŒ LLM åˆå§‹åŒ–å¤±æ•—ï¼š{e}")
        return None

def load_test_results(file_path="test_results.json"):
    """è¼‰å…¥æ¸¬è©¦çµæœæª”æ¡ˆã€‚"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            results = json.load(f)
        print(f"âœ… æˆåŠŸè¼‰å…¥æ¸¬è©¦çµæœ '{file_path}'ï¼Œå…± {len(results)} ç­†ã€‚")
        return results
    except FileNotFoundError:
        print(f"âŒ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°æ¸¬è©¦çµæœæª”æ¡ˆ '{file_path}'ã€‚è«‹å…ˆåŸ·è¡Œ 2_run_tests.pyã€‚")
        return None
    except Exception as e:
        print(f"âŒ è®€å–æ¸¬è©¦çµæœæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        return None

# --- å®šç¾©è©•ä¼°çµæœçš„è³‡æ–™çµæ§‹ ---

class EvaluationResult(BaseModel):
    """å®šç¾©å–®æ¬¡è©•ä¼°çµæœçš„è³‡æ–™çµæ§‹ã€‚"""
    accuracy_score: float = Field(description="æº–ç¢ºåº¦åˆ†æ•¸ï¼Œè¡¡é‡ç­”æ¡ˆæ˜¯å¦åŒ…å«éŒ¯èª¤æˆ–å¹»è¦ºè³‡è¨Šã€‚ç¯„åœ 0.0 åˆ° 1.0ã€‚")
    completeness_score: float = Field(description="å®Œæ•´åº¦åˆ†æ•¸ï¼Œè¡¡é‡ç­”æ¡ˆæ˜¯å¦æ¶µè“‹äº†æ‰€æœ‰é»ƒé‡‘ç­”æ¡ˆçš„è¦é»ã€‚ç¯„åœ 0.0 åˆ° 1.0ã€‚")
    explanation: str = Field(description="ä¸€æ®µç°¡çŸ­çš„æ–‡å­—ï¼Œè§£é‡‹çµ¦å‡ºåˆ†æ•¸çš„åŸå› ï¼Œä¸¦æŒ‡å‡ºå¯¦éš›ç­”æ¡ˆçš„å„ªç¼ºé»ã€‚")
    
    @field_validator('accuracy_score', 'completeness_score')
    def validate_score(cls, v):
        if not 0.0 <= v <= 1.0:
            raise ValueError('åˆ†æ•¸å¿…é ˆä»‹æ–¼ 0.0 å’Œ 1.0 ä¹‹é–“')
        return v

# --- è©•ä¼°å‡½å¼ ---

async def evaluate_single_answer_async(llm, test_result):
    """(éåŒæ­¥å‡½å¼) ä½¿ç”¨ LLM è©•ä¼°å–®ä¸€çš„å•ç­”çµæœã€‚"""
    parser = JsonOutputParser(pydantic_object=EvaluationResult)
    
    prompt_template = """
    ä½ çš„èº«ä»½æ˜¯ä¸€ä½å®¢è§€ã€åš´è¬¹ã€å¹æ¯›æ±‚ç–µçš„AIæ¨¡å‹è©•å¯©å“¡ã€‚
    ä½ çš„ä»»å‹™æ˜¯æ ¹æ“šã€Œé»ƒé‡‘æ¨™æº–ç­”æ¡ˆã€ï¼Œä¾†è©•ä¼°ã€Œå—æ¸¬ç³»çµ±çš„å¯¦éš›ç­”æ¡ˆã€çš„è¡¨ç¾ï¼Œä¸å¾—æœ‰ä»»ä½•åè¢’ã€‚

    **è©•ä¼°ç¶­åº¦:**
    1.  **æº–ç¢ºåº¦ (Accuracy)**: å¯¦éš›ç­”æ¡ˆæ˜¯å¦åŒ…å«ä»»ä½•èˆ‡é»ƒé‡‘ç­”æ¡ˆç›¸æ‚–çš„ã€éŒ¯èª¤çš„ã€æˆ–ç„¡ä¸­ç”Ÿæœ‰çš„(å¹»è¦º)è³‡è¨Šï¼Ÿå¦‚æœå®Œå…¨æº–ç¢ºï¼Œå‰‡ç‚º 1.0ï¼›å¦‚æœå®Œå…¨éŒ¯èª¤ï¼Œå‰‡ç‚º 0.0ã€‚
    2.  **å®Œæ•´åº¦ (Completeness)**: å¯¦éš›ç­”æ¡ˆæ˜¯å¦æ¶µè“‹äº†é»ƒé‡‘ç­”æ¡ˆä¸­çš„æ‰€æœ‰é—œéµè¦é»ï¼Ÿå¦‚æœå®Œå…¨æ¶µè“‹ï¼Œå‰‡ç‚º 1.0ï¼›å¦‚æœå®Œå…¨æ²’æœ‰æåˆ°ä»»ä½•è¦é»ï¼Œå‰‡ç‚º 0.0ã€‚

    **å¾…è©•ä¼°çš„è³‡æ–™å¦‚ä¸‹:**
    ---
    - **å•é¡Œ**: {question}
    - **é»ƒé‡‘æ¨™æº–ç­”æ¡ˆ (çµ•å°æ­£ç¢ºçš„åƒè€ƒä¾æ“š)**: {golden_answer}
    - **å—æ¸¬ç³»çµ±çš„å¯¦éš›ç­”æ¡ˆ (å¾…è©•ä¼°)**: {actual_answer}
    ---

    è«‹æ ¹æ“šä¸Šè¿°è©•ä¼°ç¶­åº¦ï¼Œåƒ…è¼¸å‡ºä¸€å€‹ JSON ç‰©ä»¶ï¼Œä¸å¾—æœ‰å…¶ä»–ä»»ä½•æ–‡å­—ã€‚
    {format_instructions}
    """
    
    prompt = ChatPromptTemplate.from_template(
        template=prompt_template,
        partial_variables={"format_instructions": parser.get_format_instructions()}
    )
    
    chain = prompt | llm | parser

    try:
        evaluation = await chain.ainvoke({
            "question": test_result.get("question"),
            "golden_answer": test_result.get("golden_answer"),
            "actual_answer": test_result.get("actual_answer")
        })
        # å°‡åŸå§‹è³‡æ–™èˆ‡è©•ä¼°çµæœåˆä½µ
        final_result = test_result.copy()
        final_result['evaluation'] = evaluation
        return final_result
    except Exception as e:
        print(f"âŒ è©•ä¼°å•é¡Œ '{test_result.get('question')[:20]}...' æ™‚å‡ºéŒ¯: {e}")
        final_result = test_result.copy()
        final_result['evaluation'] = {"error": str(e)}
        return final_result

async def main():
    """ä¸»åŸ·è¡Œæµç¨‹ï¼ŒåŸ·è¡Œè©•ä¼°"""
    llm_instance = initialize_llm()
    if not llm_instance:
        return

    test_results = load_test_results()
    if not test_results:
        return

    print("\n--- é–‹å§‹åŸ·è¡Œè‡ªå‹•åŒ–è©•ä¼° (åˆ†æ‰¹æ¨¡å¼) ---")
    evaluation_reports = []
    total_results = len(test_results)

    for i in range(0, total_results, BATCH_SIZE):
        batch = test_results[i:i + BATCH_SIZE]
        batch_number = (i // BATCH_SIZE) + 1
        print(f"\n--- æ­£åœ¨è©•ä¼°ç¬¬ {batch_number} æ‰¹æ¬¡ (çµæœ {i+1} åˆ° {min(i + BATCH_SIZE, total_results)}) ---")

        tasks = [evaluate_single_answer_async(llm_instance, result) for result in batch]
        batch_evaluations = await asyncio.gather(*tasks)
        
        evaluation_reports.extend(batch_evaluations)

        if i + BATCH_SIZE < total_results:
            print(f"--- ç¬¬ {batch_number} æ‰¹æ¬¡è©•ä¼°å®Œç•¢ï¼Œä¼‘æ¯ {DELAY_BETWEEN_BATCHES} ç§’ ---")
            await asyncio.sleep(DELAY_BETWEEN_BATCHES)

    output_filename = "evaluation_report.json"
    try:
        with open(output_filename, 'w', encoding='utf-8') as f:
            json.dump(evaluation_reports, f, ensure_ascii=False, indent=4)
        print(f"\n\nğŸ‰ è©•ä¼°å…¨éƒ¨å®Œæˆï¼å ±å‘Šå·²å„²å­˜è‡³ '{output_filename}'")
    except Exception as e:
        print(f"âŒ å„²å­˜è©•ä¼°å ±å‘Šæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

if __name__ == "__main__":
    asyncio.run(main())
